{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy\n",
    "import codecs\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import operator as op\n",
    "\n",
    "fp = sys.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing starts...\n",
      "processing.           \n",
      "phase 1 done.\n",
      "recleaning phase done.\n",
      "\n",
      "statistics:\n",
      "\ttotal number of users:                     10000\n",
      "\tusers read in the first 20 days:           9223\n",
      "\tusers read in the last 10 days:            3201\n",
      "\tusers read in both two periods:            2429\n",
      "\tusers read at least 3 news in each period: 341\n",
      "\tnumber of valid news:                      5866\n",
      "\tnumber of news read by valid users:        3309\n"
     ]
    }
   ],
   "source": [
    "def preprocess():\n",
    "    f = codecs.open('./data/user_click_data.txt', 'r', 'utf-8')\n",
    "#     f_training = codecs.open('./data/_training.json', 'w', 'utf-8')\n",
    "#     f_validation = codecs.open('./data/_validation.json', 'w', 'utf-8')\n",
    "    f_user_data_training = codecs.open('./data/_user_data_training.json', 'w', 'utf-8')\n",
    "    f_user_data_validation = codecs.open('./data/_user_data_validation.json', 'w', 'utf-8')\n",
    "    f_news_data = codecs.open('./data/_news_data.json', 'w', 'utf-8')\n",
    "    f_user_data_training_clean = codecs.open('./data/_user_data_training_clean.json', 'w', 'utf-8')\n",
    "    f_user_data_validation_clean = codecs.open('./data/_user_data_validation_clean.json', 'w', 'utf-8')\n",
    "    f_news_data_clean = codecs.open('./data/_news_data_clean.json','w','utf-8')\n",
    "    f_news_data_1to20_clean = codecs.open('./data/_news_data_1to20_clean.json', 'w', 'utf-8')\n",
    "    f_news_data_15to30_clean = codecs.open('./data/_news_data_15to30_clean.json', 'w', 'utf-8')\n",
    "    i=0\n",
    "    user_data_training = {}\n",
    "    news_data = {}\n",
    "    user_data_validation = {}\n",
    "    user_data_training_clean = {}\n",
    "    user_data_validation_clean = {}\n",
    "    news_data_clean={}\n",
    "    news_data_1to20_clean={}\n",
    "    news_data_15to30_clean={}\n",
    "    #news_data_validation = {}\n",
    "    print(\"preprocessing starts...\")\n",
    "    for line in f:\n",
    "        # progress bar\n",
    "        i+=1\n",
    "        if(i % 10000 == 20):\n",
    "            fp.write('\\r')\n",
    "            fp.write(\"processing\")\n",
    "            for z in range(int(i / 10000)%5):\n",
    "                fp.write(\".\")\n",
    "            fp.write(\"        \")\n",
    "        partitions = line.split('\\t')\n",
    "        # user_id, news_id, click_time, title, article, news_time\n",
    "        user_id = int(partitions[0])\n",
    "        news_id = int(partitions[1])\n",
    "        click_time = int(partitions[2])\n",
    "        # delete all dirty record\n",
    "        if partitions[4] == 'NULL' or partitions[3] == '404':\n",
    "#         if partitions[3] == '404':\n",
    "            continue\n",
    "        # deal with news_time\n",
    "        try:\n",
    "            tstp = transform_time(partitions[5])  \n",
    "        except:\n",
    "#             continue\n",
    "            tstp = int(1393603200)\n",
    "        # data = {\"user_id\": user_id, \"news_id\": news_id, \"click_time\": click_time,\n",
    "        #        \"title\": partitions[3], \"article\": partitions[4], \"news_time\": tstp}\n",
    "        day = int((int(partitions[2]) - 1393603200) / 86400) + 1\n",
    "        if day <= 20:\n",
    "            # the first 20 days records belong to training set\n",
    "#             json.dump(data, f_training)\n",
    "            if user_id not in user_data_training:\n",
    "                user_data_training.setdefault(user_id, {})\n",
    "            user_data_training[user_id].setdefault(news_id, click_time)\n",
    "        else:\n",
    "            # the last 10 days records belong to validation set\n",
    "#             json.dump(data, f_validation)\n",
    "            if user_id not in user_data_validation:\n",
    "                user_data_validation.setdefault(user_id, {})\n",
    "            user_data_validation[user_id].setdefault(news_id, click_time)\n",
    "        \n",
    "        if news_id in news_data:\n",
    "            if len(partitions[4]) > len(news_data[news_id][1]):\n",
    "                # if necessary,\n",
    "                # update the news info\n",
    "                news_data[news_id][1] = partitions[4]\n",
    "                news_data[news_id][2] = tstp\n",
    "#             if op.eq(partitions[4], news_data[news_id][1]) != 0 and \\\n",
    "#             op.eq(partitions[3], news_data[news_id][0]) != 0 and news_data[news_id][2] != tstp:\n",
    "        else:\n",
    "            # [news_title, news_article, news_time]\n",
    "            news_data.setdefault(news_id, [partitions[3], partitions[4], tstp])\n",
    "    \n",
    "    fp.write('\\nphase 1 done.\\n')\n",
    "    user_validation = user_data_validation.keys()\n",
    "    user_training = user_data_training.keys()\n",
    "#     zombie = list(set(user_validation) ^ set(user_training))\n",
    "#     ret1 = [ i for i in user_validation if i not in user_training ]\n",
    "#     ret2 = [ i for i in user_training if i not in user_validation ]\n",
    "#     user_list = [ i for i in user_training if i in user_validation ]\n",
    "#     print(len(zombie), len(ret1), len(ret2), len(user_validation), len(user_training), len(user_list))\n",
    "    # we only concern those who read news in both periods\n",
    "    user_list = [ i for i in user_training if i in user_validation ]\n",
    "    news_list = []\n",
    "    z = 0\n",
    "    for i in user_list:\n",
    "        # those who read little news in one period are also not considered\n",
    "        if len(user_data_validation[i]) >= 5 and len(user_data_training[i]) >= 5:\n",
    "            z+=1\n",
    "            user_data_validation_clean.setdefault(i, user_data_validation[i])\n",
    "            user_data_training_clean.setdefault(i, user_data_training[i])\n",
    "            for n in user_data_validation[i]:\n",
    "                if n not in news_list:\n",
    "                    news_list.append(n)\n",
    "            for n in user_data_training[i]:\n",
    "                if n not in news_list:\n",
    "                    news_list.append(n)\n",
    "    for n in news_list:\n",
    "        news_data_clean.setdefault(n, news_data[n])\n",
    "        zz=int(news_data[n][2] - 1393603200)/86400\n",
    "        if zz <= 20:\n",
    "            news_data_1to20_clean.setdefault(n,news_data[n])\n",
    "        if zz > 15:\n",
    "            news_data_15to30_clean.setdefault(n, news_data[n])\n",
    "            \n",
    "    print(\"recleaning phase done.\")\n",
    "    print(\"\\nstatistics:\")\n",
    "    print(\"\\ttotal number of users:                     10000\")\n",
    "    print(\"\\tusers read in the first 20 days:          \", len(user_data_training))\n",
    "    print(\"\\tusers read in the last 10 days:           \", len(user_data_validation))\n",
    "    print(\"\\tusers read in both two periods:           \", len(user_list))\n",
    "    print(\"\\tusers read at least 3 news in each period:\", z)\n",
    "    print(\"\\tnumber of valid news:                     \", len(news_data))\n",
    "    print(\"\\tnumber of news read by valid users:       \", len(news_data_clean))\n",
    "    \n",
    "#     z=0\n",
    "#     f = codecs.open('./data/user_click_data.txt', 'r', 'utf-8')\n",
    "#     for line in f:\n",
    "#         partitions = line.split('\\t')\n",
    "#         if int(partitions[0]) in user_list:\n",
    "#             z+=1\n",
    "#     print(z)\n",
    "    # dump into files\n",
    "    json.dump(user_data_training_clean, f_user_data_training_clean)\n",
    "    json.dump(user_data_validation_clean, f_user_data_validation_clean)\n",
    "    json.dump(user_data_training, f_user_data_training)\n",
    "    json.dump(user_data_validation, f_user_data_validation)\n",
    "    json.dump(news_data, f_news_data)\n",
    "    json.dump(news_data_clean, f_news_data_clean)\n",
    "    json.dump(news_data_1to20_clean, f_news_data_1to20_clean)\n",
    "    json.dump(news_data_15to30_clean, f_news_data_15to30_clean)\n",
    "    # close files\n",
    "#     f_training.close()\n",
    "#     f_validation.close()\n",
    "    f_user_data_training.close()\n",
    "    f_user_data_validation.close()\n",
    "    f_news_data.close()\n",
    "    f_user_data_training_clean.close()\n",
    "    f_user_data_validation_clean.close()\n",
    "    f_news_data_clean.close()\n",
    "    f.close()\n",
    "    \n",
    "def transform_time(t):\n",
    "    # t :\n",
    "    # 2014年03月xx日xx:xx\n",
    "    tmp = t.split(\"年\")\n",
    "    year = str(tmp[0])\n",
    "    tmp = tmp[1].split(\"月\")\n",
    "    mon = str(tmp[0])\n",
    "    tmp = tmp[1].split(\"日\")\n",
    "    day = str(tmp[0])\n",
    "    tmp = tmp[1].split(\":\")\n",
    "    hour = str(tmp[0])\n",
    "    minute = str(tmp[1].split(\"\\r\")[0])\n",
    "    # 1393603200 >>> 2014-03-01 00:00:00\n",
    "    a = year+'-'+mon+'-'+day+' '+hour+':'+minute+':'+'00'\n",
    "    timeArray = time.strptime(a, \"%Y-%m-%d %H:%M:%S\")\n",
    "    timeStamp = int(time.mktime(timeArray))\n",
    "    return int(timeStamp)\n",
    "\n",
    "preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1393603200\n"
     ]
    }
   ],
   "source": [
    "a = \"2014-03-01 00:00:00\"\n",
    "timeArray = time.strptime(a, \"%Y-%m-%d %H:%M:%S\")\n",
    "timeStamp = int(time.mktime(timeArray))\n",
    "print(timeStamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2014-03-01 00:00:00\n",
    "    ↓\n",
    "    1393603200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1393737720\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = \"2014年03月02日13:22\"\n",
    "print(transform_time(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
