{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import json\n",
    "import codecs\n",
    "import jieba\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def news_vector_dict(title_scale, doc_scale):\n",
    "    file = codecs.open('_news_data.json', 'r', 'utf-8')\n",
    "    news_dict = json.load(file)\n",
    "    \n",
    "    # 分词，在词之间加空格，重新组成文章\n",
    "    i = 0\n",
    "    title_array = []\n",
    "    doc_array = []\n",
    "    for news_key in news_dict:\n",
    "        title_text = news_dict[news_key][0]\n",
    "        doc_text = news_dict[news_key][1]\n",
    "        title = ' '.join(jieba.lcut(title_text))\n",
    "        doc = ' '.join(jieba.lcut(doc_text))\n",
    "        title_array.append(title)\n",
    "        doc_array.append(doc)\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "    \n",
    "    # tf-idf算法，文章转化为一个归一化的向量\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df = 10)\n",
    "    doc_matrix = tfidf_vectorizer.fit_transform(doc_array)\n",
    "    title_matrix = tfidf_vectorizer.transform(title_array)\n",
    "    \n",
    "    # 计算文章加权vector\n",
    "    news_matrix = (title_matrix.todense() * title_scale + doc_matrix.todense() * doc_scale).tolist()\n",
    "    \n",
    "    # 构建news_key : vector字典\n",
    "    i = 0\n",
    "    news_vector_dict = {}\n",
    "    for news_key in news_dict:\n",
    "#         news_vector = title_matrix[i].todense()\n",
    "        news_vector_dict.setdefault(news_key, news_matrix[i])\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print('i='+str(i))\n",
    "            print(news_matrix[i][:10])\n",
    "    # file_output = codecs.open('_news_data_tfidf.json', 'w', 'utf-8')\n",
    "    # json.dump(news_vector_dict, file_output)\n",
    "    # print(tfidf_vectorizer.vocabulary_) \n",
    "    \n",
    "    return news_vector_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_vector_dict(news_vector_dict, time_scalse):\n",
    "    file = codecs.open('_user_data_training.json', 'r', 'utf-8')\n",
    "    user_dict = json.load(file)\n",
    "    \n",
    "    j = 0\n",
    "    user_vector_dict = {}\n",
    "    # 每一个用户\n",
    "    for user_key in user_dict:\n",
    "        # 该用户读过的所有新闻的向量和为用户向量\n",
    "        i = 0\n",
    "        vector_sum = numpy.matrix('0.0')\n",
    "        for user_news_key in user_dict[user_key]:\n",
    "            vector = numpy.matrix(news_vector_dict[user_news_key])\n",
    "            vector_sum = vector * time_scale + vector_sum\n",
    "            i += time_scale\n",
    "        if i != 0:\n",
    "            vector_sum /= i\n",
    "        user_vector_dict.setdefault(user_key, vector_sum.tolist()[0])\n",
    "        j += 1\n",
    "        if j % 1000 == 0:\n",
    "            print('j='+str(j))\n",
    "            print(vector_sum.tolist()[0][:10])\n",
    "    return user_vector_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_n_n(news_dict, user_dict, k):\n",
    "    news_keys = []\n",
    "    news = []\n",
    "    i = 0\n",
    "    for news_key in news_dict:\n",
    "        news_keys.append(news_key)\n",
    "        news.append(news_dict[news_key])\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "    \n",
    "    print(\"training...\")\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(news)\n",
    "    \n",
    "    user_keys = []\n",
    "    nbrs = []\n",
    "    i = 0\n",
    "    for user_key in user_dict:\n",
    "        users = []\n",
    "        user_keys.append(user_key)\n",
    "        users.append(user_dict[user_key])\n",
    "        nbrs += neigh.kneighbors(users)\n",
    "        i += 1\n",
    "        if i % 50 == 0:\n",
    "            print(i)\n",
    "    \n",
    "#     n = neigh.kneighbors(users)\n",
    "    print(nbrs[:10])\n",
    "    return nbrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\49325\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 2.297 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "i=1000\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "i=2000\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "i=3000\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "i=4000\n",
      "[0.10698909037437322, 0.029850505179614607, 0.0, 0.0028634010684771313, 0.001411267289966212, 0.0028681237181149596, 0.002877685310018286, 0.0, 0.0014270344823457477, 0.00146138260933671]\n",
      "i=5000\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "i=6000\n",
      "[0.027122530457869668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "title_scale = 0.5\n",
    "doc_scale = 1.0 - title_scale\n",
    "time_scale = 1.0\n",
    "k = 10\n",
    "news_vector_dict = news_vector_dict(title_scale, doc_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=1000\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "j=2000\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "j=3000\n",
      "[0.0, 0.0, 0.02398540266471601, 0.0007732613785927483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "j=4000\n",
      "[0.0, 0.0, 0.0, 0.002411076522521961, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "j=5000\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "j=6000\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "j=7000\n",
      "[0.0, 0.0, 0.02098622890012179, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "j=8000\n",
      "[0.0, 0.0, 0.006535560877925954, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "j=9000\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "user_vector_dict = user_vector_dict(news_vector_dict, time_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "training...\n",
      "1000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "n = k_n_n(news_vector_dict, user_vector_dict, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
