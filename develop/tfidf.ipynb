{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import json\n",
    "import codecs\n",
    "import jieba\n",
    "import numpy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "i=0\n",
      "-------------------------\n",
      "i=1000\n",
      "-------------------------\n",
      "i=2000\n",
      "-------------------------\n",
      "i=3000\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "def news_vector_dict(file_root, title_scale, doc_scale, min_df, max_df):\n",
    "    file = codecs.open(file_root, 'r', 'utf-8')\n",
    "    news_dict = json.load(file)\n",
    "    \n",
    "    # 停用词表\n",
    "    stop_file = codecs.open('./data/stop_words.txt', 'r', 'utf-8')\n",
    "    stop_list = stop_file.read().split('\\n')\n",
    "    stop_file.close()\n",
    "    \n",
    "    # 分词，在词之间加空格，重新组成文章\n",
    "    i = 0\n",
    "    title_array = []\n",
    "    doc_array = []\n",
    "    for news_key in news_dict:\n",
    "        title_text = news_dict[news_key][0]\n",
    "        _title = jieba.lcut(title_text)\n",
    "        for w in _title[:]:\n",
    "            if w.split('.')[0].isdigit():\n",
    "                _title.remove(w)\n",
    "        title = ' '.join(_title)\n",
    "        title_array.append(title)\n",
    "        \n",
    "        doc_text = news_dict[news_key][1]\n",
    "        _doc = jieba.lcut(doc_text)\n",
    "        for w in _doc[:]:\n",
    "            if w.split('.')[0].isdigit():\n",
    "                _doc.remove(w)\n",
    "        doc = ' '.join(_doc)\n",
    "        doc_array.append(doc)\n",
    "        \n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "    \n",
    "    scale = max(int(title_scale / doc_scale), 1)\n",
    "    doc_array = title_array * scale + doc_array\n",
    "    \n",
    "    # tf-idf算法，文章转化为一个归一化[并没有]的向量\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df = min_df, max_df = max_df, stop_words = stop_list)\n",
    "    tfidf_vectorizer.fit(doc_array)\n",
    "    doc_matrix = tfidf_vectorizer.transform(doc_array)\n",
    "    news_matrix = doc_matrix.todense().tolist()\n",
    "    \n",
    "#     word_bag = {}\n",
    "#     for key in tfidf_vectorizer.vocabulary_:\n",
    "#         word_bag.setdefault(tfidf_vectorizer.vocabulary_[key], key)\n",
    "    \n",
    "    # 构建news_key : vector字典\n",
    "    i = 0\n",
    "    news_vector_dict = {}\n",
    "    for news_key in news_dict:\n",
    "        news_vector_dict.setdefault(news_key, news_matrix[i])\n",
    "        if i % 1000 == 0:\n",
    "            print('i='+str(i))\n",
    "            \n",
    "         #打印文章关键词和权重\n",
    "#         if i < 15:\n",
    "#             news_words = []\n",
    "#             news_words_weight = []\n",
    "#             for j in range(len(news_matrix[i])):\n",
    "#                 if news_matrix[i][j] > 0:\n",
    "#                     news_words.append(word_bag[j])\n",
    "#                     news_words_weight.append(news_matrix[i][j])\n",
    "#             print(news_words)\n",
    "#             print(title_array[i])\n",
    "#             print(doc_array[i])\n",
    "#             print(news_words_weight)\n",
    "\n",
    "            print('-------------------------')\n",
    "    \n",
    "        i += 1\n",
    "\n",
    "    return news_vector_dict\n",
    "\n",
    "file_root = './data/_news_data_clean.json'\n",
    "# NOTE: scale = MAX(int(title_scale / doc_scale), 1)\n",
    "title_scale = 0.5\n",
    "doc_scale = 1.0 - title_scale\n",
    "min_df = 3\n",
    "max_df = 25\n",
    "_news_vector_dict = news_vector_dict(file_root, title_scale, doc_scale, min_df, max_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_back(t):\n",
    "    a = int(t-1393603200)\n",
    "    return float(a / 86400 / 20)\n",
    "\n",
    "def user_vector_dict(news_vector_dict):\n",
    "    file = codecs.open('./data/_user_data_training_clean.json', 'r', 'utf-8')\n",
    "    news_data = codecs.open('./data/_news_data_clean.json', 'r', 'utf-8')\n",
    "    user_dict = json.load(file)\n",
    "    news_d = json.load(news_data)\n",
    "    \n",
    "    j = 0\n",
    "    user_vector_dict = {}\n",
    "    # 每一个用户\n",
    "    for user_key in user_dict:\n",
    "        # 该用户读过的所有新闻的向量和为用户向量\n",
    "        i = 0\n",
    "        vector_sum = numpy.matrix('0.0')\n",
    "        for user_news_key in user_dict[user_key]:\n",
    "            vector = numpy.matrix(news_vector_dict[user_news_key])\n",
    "#             time_scale = time_back(news_d[user_news_key][2])\n",
    "            time_scale = 1\n",
    "            vector_sum = vector * time_scale + vector_sum\n",
    "            i += 1\n",
    "        if i != 0:\n",
    "            vector_sum /= i\n",
    "        user_vector_dict.setdefault(user_key, vector_sum.tolist()[0])\n",
    "        j += 1\n",
    "        if j % 1000 == 0:\n",
    "            print('j='+str(j))\n",
    "            print(vector_sum.tolist()[0][:10])\n",
    "    return user_vector_dict\n",
    "\n",
    "\n",
    "_user_vector_dict = user_vector_dict(_news_vector_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "training...\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "[array([[ 0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588,\n",
      "         0.12263588,  0.12263588,  0.12263588,  0.12263588,  0.12263588]]), array([[ 137,  853,  847,  443,  844, 1023,  504,  437,  924,  842,  440,\n",
      "        1008,  475,  482,  471,  498, 1237, 1004,  831,  834,  484,  497,\n",
      "         481, 1005,  464,  520,  476,  636,  644,  597,  601,  587,  607,\n",
      "        1011,  444,  933, 1022,  951, 1017, 1025,  556,  612,  495,  542,\n",
      "         544,  550,  210,  557,  808,  330,  822,  639,  470,  332,  632,\n",
      "         630,  355,  733,  712,  517,  723,  574,  302,  576,  371, 3308,\n",
      "         468,  524,  308,  369,  445,  932, 1020, 1013,   38,   37,   47,\n",
      "         987, 1018,  986,  392,  378,  383,  604,  543,  776,  540,  530,\n",
      "         783,  397,  793,  779,  110,  282,  284,  545,  128,  318,  317,\n",
      "         312,  140,  323,  321,  810,  216,  333,  331,  151,  719,  357,\n",
      "         618,  251,  264,  223,  168,  167,  220,  698,  695,  664,  304,\n",
      "         222,  192,  722,  185,  300,  514,  190,  226,  291,  824, 1652,\n",
      "           8,  225,    6,    5,  367,  241,   23,   14,  268,  438,   22,\n",
      "          20,   25,  953,  931,  996,   34,   33,   35,   36, 1651,   42,\n",
      "          43,  454,  975,  954,  977,  976,  231,  214,   63,  240,   65,\n",
      "         272,  305,   71,  528,   75,  385,  279,  775,  228,  772,   84,\n",
      "          89,  211,   94,  396,   46,  257,   98,  306,  104,  106,  107,\n",
      "         109,  111,  113,  114,  407,  122,  124,  248,  224,  267,  366,\n",
      "         262,  130]], dtype=int64), array([[ 0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836]]), array([[ 137, 1203, 1200, 1173, 1196, 1176, 1119, 1107, 1194, 1183, 1147,\n",
      "        1091, 1063,  636,  698, 1059, 1155, 1178, 1141,  504, 1100,  723,\n",
      "         772,  733,  639,  719,  438,  632,  464,  695, 1058, 1048, 1122,\n",
      "         481, 1175, 1159, 1070,  484,  497,  528,  332, 1074,  248,  722,\n",
      "         331,  576,  574,  557,  793,  783,  810,  822,  604,  367,  437,\n",
      "         630,  444,  443,  476,  664,  475,  712,  471,  470,  514,  333,\n",
      "         517,  597, 1158, 1143, 1145, 1148,  775,  524,  540,  520,  225,\n",
      "         495,  223,  224,  241,  312,  543,  587,   34,  264,  545,  305,\n",
      "          38,  321,  318,  556,  110,  268,  267,  330,  128,  601,  378,\n",
      "         779,  140,  808,  355,  357,  279,  369,  366,  151,  607,  371,\n",
      "         612,  618,  383,  284,  168,  167,  445,  454,  385,  644,  291,\n",
      "         392,  396,  397,  185,  192,  468,  190,  262,   47,  257,  228,\n",
      "         304,  300,  302,  542,   75,   63,  482,   65, 1095, 1101,   71,\n",
      "        1144,  216,  498,  210,  211,   84,  226,  214,   89,   94,  220,\n",
      "         530,   46,  824,  222, 1652, 3308,    5,    6,    8,  231,   14,\n",
      "          35,  544,  240,   20,   22,   23,   25,   33,  308,  251,  306,\n",
      "          36,   37,   42, 1651,   43,  317,   98,  550,  104,  106,  107,\n",
      "         109,  111,  113,  114,  323,  122,  124,  282,  407,  272,  440,\n",
      "         776,  130]], dtype=int64), array([[ 0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622,\n",
      "         0.30618622,  0.30618622,  0.30618622,  0.30618622,  0.30618622]]), array([[ 137,  977,  976,  932,  975,  877, 1028,  900,  910,  954,  863,\n",
      "         869,  776,  890,  893,  853,  887,  639,  953,  543,  545,  636,\n",
      "         632,  864,  644,  712,  775,  862,  808,  858,  924,  847,  497,\n",
      "         445,  504,  444,  520,  951,  542,  530,  304,  544,  587,  597,\n",
      "         601,  612,  220,  630,  257,  481,  698,  695,  722,  723,  733,\n",
      "         772,  783,  793, 1237,  822,  831,  834,  476,  844,  396,  482,\n",
      "         484,  495,  392,  498,  443,  440,  517,  514,  931,  933,  524,\n",
      "         540,  308,  528,  330,  323,  333,  223,   34,  550,  576,  574,\n",
      "          38,  222,  607,  604,  110,  214,  618,  719,  128,  241,  240,\n",
      "         284,  140,  664,  248,  251,  262,  267,  264,  151,  282,  268,\n",
      "         272,  279,  464,  779,  168,  167,  291,  810,  300,  302,  192,\n",
      "         471,  468,  470,  185,  475,  190,  842,  378,   47,  318,  224,\n",
      "         367,  366,  371,  369,   63,  383,  385,   65,  437,   75,   71,\n",
      "         438,  407,  317,  397,  454,   84,  920,   89,  921,   94,  306,\n",
      "         312,   46,  824,  305, 1652, 3308,    5,    6,    8,  321,   14,\n",
      "          35,  331,  332,   20,   22,   23,   25,   33,  355,  556,  557,\n",
      "          36,   37,   42, 1651,   43,  210,   98,  211,  104,  106,  107,\n",
      "         109,  111,  113,  114,  216,  122,  124,  357,  225,  226,  228,\n",
      "         231,  130]], dtype=int64), array([[ 0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558,\n",
      "         0.19509558,  0.19509558,  0.19509558,  0.19509558,  0.19509558]]), array([[ 323, 1203, 1200, 1175, 1196, 1183, 1119, 1107, 1194,  612, 1145,\n",
      "        1155, 1095,  530,  604, 1101, 1178, 1176,  231,  282,  783, 1143,\n",
      "        1122, 1148, 1091,  607,  517,  528,  543,  550,  601, 1100,  284,\n",
      "         355, 1173, 1159,  224,  228,  257,  279,  291,  779, 1141,  454,\n",
      "        1070,  445, 1063, 1059,  476,  497,  484,  540,  367,  514, 3308,\n",
      "         524,   20,  542,  544,  545,  556,  557,  597,  587,  128,  216,\n",
      "         111,   38,  214, 1147,  211, 1158,  220,  223,  225,  226,  241,\n",
      "         251,  264,  272,  407,  317,  772,  776, 1048,  822,  397,  733,\n",
      "         719,  644,  636,  639, 1058,  698,  443,  440,  464,  468,  475,\n",
      "         471,  481,  482,   89,  495,   94,   46,  498,  504,  824, 1652,\n",
      "         366,  520,    6,    8,  333,   36,  332,   14,  331,  330,   23,\n",
      "          22,   35,   25,  574,   34,  576,   33,  104,   98,  304,  318,\n",
      "         107,  106,  110,  109,  113,  114, 1144, 1074,  122,  371,  210,\n",
      "         124,   42,  130,  222,   43,  378,  137, 1651,  140,  385,  240,\n",
      "         151,  248,  383,  262,  268,  267,  192,  168,  357,  167,  300,\n",
      "         302,  392,  775,  396,  793,  810,  808,  190,  185,  722,  723,\n",
      "         305,  312,  306,  618,  630,  632,   47,  308,  444,  712,  664,\n",
      "         695,   65,  437,  438,   63,    5,   71,   75,   37,  321,  470,\n",
      "         369,   84]], dtype=int64), array([[ 0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836,\n",
      "         0.19633836,  0.19633836,  0.19633836,  0.19633836,  0.19633836]]), array([[ 385, 1159, 1158, 1155, 1147, 1143, 1148, 1107, 1196, 1144, 1059,\n",
      "        1141, 1145,  476,  498,  495,  355,  576, 1222, 1214, 1048, 1058,\n",
      "         639, 1231, 1095,  497,  540,  444,  544,  443,  484,  482,  323,\n",
      "         304,  793,  333, 1217, 1206,  783,  779,  722, 1101, 1122,  454,\n",
      "         719,  636,  471, 1091,  504,  520,  517,  542,   37,  528, 3308,\n",
      "         530,   20,  543,  545,  556,  312,  557,  440,  308,  321,  318,\n",
      "         111,   38,  128,  330,  331,  332, 1194, 1074, 1203, 1200,  775,\n",
      "         222,  226,  776,  369,  282,  822,  810, 1100,  733,  279,  574,\n",
      "         695,  712,  630,  632,  357,  644,  468,  470,  317,  481,  604,\n",
      "         601,  612,  607,   89,  514,   94,   46,  397,  524,  824, 1652,\n",
      "         392,  396,    6,    8,   36,  305,  306,   14,  550,  407,   23,\n",
      "          22,   35,   25,  437,  438,   33,   34,  241,   98,  104,  371,\n",
      "         107,  106,  110,  109,  113,  114,  211,  210,  216,  122,  214,\n",
      "         124, 1070,  130, 1063,  445, 1119,  137,  240,  140,  772,  223,\n",
      "         151,  220,  224,  225,  228,  231,  264,  168,  248,  167,  262,\n",
      "         251,  808,  257,  475,  267,  723,  268,  190,  185,  192,  272,\n",
      "         284,  367,   43,  698,  300,  618,   47,  291,   42, 1651,  302,\n",
      "         366,   65,  664,  464,   63,   84,   71,   75,  383,  597,    5,\n",
      "         378,  587]], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "def k_n_n(news_dict, user_dict, k):\n",
    "    news_keys = []\n",
    "    news = []\n",
    "    i = 0\n",
    "    for news_key in news_dict:\n",
    "        news_keys.append(news_key)\n",
    "        news.append(news_dict[news_key])\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "    \n",
    "    print(\"training...\")\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(news)\n",
    "    \n",
    "    user_keys = []\n",
    "    nbrs = []\n",
    "    i = 0\n",
    "    for user_key in user_dict:\n",
    "        users = []\n",
    "        user_keys.append(user_key)\n",
    "        users.append(user_dict[user_key])\n",
    "        nbrs += neigh.kneighbors(users)\n",
    "        i += 1\n",
    "        if i % 50 == 0:\n",
    "            print(i)\n",
    "    \n",
    "#     n = neigh.kneighbors(users)\n",
    "    print(nbrs[:10])\n",
    "    return nbrs\n",
    "\n",
    "k = 200\n",
    "n = k_n_n(_news_vector_dict, _user_vector_dict, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[117, 145, 154, 165, 171, 173, 174, 174, 181, 183]\n"
     ]
    }
   ],
   "source": [
    "# def time_scale(t):\n",
    "#     day=int((t-1393603200)/86400)\n",
    "#     if day < 1:\n",
    "#         day = 1\n",
    "#     return (math.log(day)+1)\n",
    "\n",
    "# news_keys = []\n",
    "# i = 0\n",
    "# for news_key in _news_vector_dict:\n",
    "#     news_keys.append(news_key)\n",
    "\n",
    "    \n",
    "# file = codecs.open('./data/_user_data_training_clean.json', 'r', 'utf-8')\n",
    "# f_news_data = codecs.open('./data/_news_data_clean.json', 'r', 'utf-8')\n",
    "# user_news_dict = json.load(file)\n",
    "# news_data = json.load(f_news_data)\n",
    "\n",
    "# result = {}\n",
    "# i = 0\n",
    "# lens = []\n",
    "# pair = []\n",
    "# for user_key in user_news_dict:\n",
    "#     dist = n[2*i][0].tolist()\n",
    "#     indices = n[2*i+1][0].tolist()\n",
    "#     pair = []\n",
    "#     for m in range(len(indices)):\n",
    "#         mth = indices[m]\n",
    "#         news_id = news_keys[mth]\n",
    "#         if news_id in user_news_dict[user_key]:\n",
    "#             continue\n",
    "    \n",
    "#         time_ratio = time_scale(news_data[news_id][2])\n",
    "#         #print(time_ratio, dist[m], dist[m]*time_ratio)\n",
    "#         pair.append([dist[m] * time_ratio, news_id])\n",
    "    \n",
    "#     pair.sort(key=lambda x:x[0],reverse=True)\n",
    "#     user_news_keys = []\n",
    "#     for k in range(30):\n",
    "#         user_news_keys.append(pair[k][1])\n",
    "#     result.setdefault(user_key, user_news_keys)\n",
    "#     if i < 10:\n",
    "#         for p in pair:\n",
    "#             print(p[0], time_scale(news_data[p[1]][2]))\n",
    "#         print(\"\\n\")\n",
    "    \n",
    "# #         print(len(result[user_key]))\n",
    "# #         print(result[user_key])\n",
    "# #         print(user_news_dict[user_key])\n",
    "#     i += 1\n",
    "#     lens.append(len(result[user_key]))\n",
    "# lens.sort()\n",
    "# print(lens[:10])\n",
    "\n",
    "\n",
    "\n",
    "news_keys = []\n",
    "i = 0\n",
    "for news_key in _news_vector_dict:\n",
    "    news_keys.append(news_key)\n",
    "\n",
    "    \n",
    "file = codecs.open('./data/_user_data_training_clean.json', 'r', 'utf-8')\n",
    "user_news_dict = json.load(file)\n",
    "\n",
    "result = {}\n",
    "i = 0\n",
    "lens = []\n",
    "for user_key in user_news_dict:\n",
    "    indices = n[2*i+1][0].tolist()\n",
    "    user_news_keys = []\n",
    "    for index in indices:\n",
    "        user_news_key = news_keys[index]\n",
    "        if user_news_key not in user_news_dict[user_key]:\n",
    "            user_news_keys.append(user_news_key)\n",
    "    result.setdefault(user_key, user_news_keys)\n",
    "#     if i < 100:\n",
    "#         print(len(result[user_key]))\n",
    "#         print(result[user_key])\n",
    "#         print(user_news_dict[user_key])\n",
    "    i += 1\n",
    "    lens.append(len(result[user_key]))\n",
    "lens.sort()\n",
    "print(lens[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_output = codecs.open('./data/tfidf_result.json', 'w', 'utf-8')\n",
    "json.dump(result, file_output)\n",
    "file_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "846 66720 674\n",
      "precision:  0.010621749434394432\n",
      "recall:  0.12674925934091577\n"
     ]
    }
   ],
   "source": [
    "def time_back(t):\n",
    "    a = int(t-1393603200)\n",
    "    return int(a / 86400)\n",
    "\n",
    "def test(result_root):\n",
    "    f_user_data_validation = codecs.open('./data/_user_data_validation_clean.json', 'r', 'utf-8')\n",
    "    f_result = codecs.open(result_root, 'r', 'utf-8')\n",
    "    f_news_data = codecs.open('./data/_news_data.json', 'r', 'utf-8')\n",
    "    f_user_data_training = codecs.open('./data/_user_data_training_clean.json', 'r', 'utf-8')\n",
    "    training = json.load(f_user_data_training)\n",
    "    validation = json.load(f_user_data_validation)\n",
    "    result = json.load(f_result)\n",
    "    news_data = json.load(f_news_data)\n",
    "    \n",
    "    z=0\n",
    "    q=0\n",
    "    user_num = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    sum=0\n",
    "    for key in validation:\n",
    "        user_num += 1\n",
    "        if key in result:\n",
    "            rec_num = len(result[key])\n",
    "        else:\n",
    "            continue\n",
    "        act_num = len(validation[key])\n",
    "        TP = 0\n",
    "        for news_id in result[key]:\n",
    "            q+=1\n",
    "    #         print(news_data[news_id][0])\n",
    "            if news_id in validation[key]:\n",
    "                TP+=1\n",
    "\n",
    "#         print(key)\n",
    "#         for a in validation[key]:\n",
    "#             print(news_data[a][0])\n",
    "#         print(\"\\n\")\n",
    "#         for a in training[key]:\n",
    "#             print(news_data[a][0])\n",
    "#         print(\"\\n\")\n",
    "        sum+=TP\n",
    "        for a in validation[key]:\n",
    "            if time_back(news_data[a][2]) < 10:\n",
    "                z+=1\n",
    "#         if TP == 0:\n",
    "#             print(key)\n",
    "#             for a in result[key]:\n",
    "#                 print(news_data[a][0], time_back(news_data[a][2]))\n",
    "#             print(\"\\n\")\n",
    "#             for a in validation[key]:\n",
    "#                 print(news_data[a][0], time_back(news_data[a][2]),time_back(validation[key][a]))\n",
    "#             print(\"\\n\")\n",
    "#             for a in training[key]:\n",
    "#                 print(news_data[a][0], time_back(news_data[a][2]),time_back(training[key][a]))\n",
    "#             print(\"\\n\")\n",
    "#             break\n",
    "    \n",
    "#         print(precision, recall)\n",
    "        precision += TP / rec_num\n",
    "        recall += TP / act_num\n",
    "    precision = precision / user_num \n",
    "    recall = recall / user_num\n",
    "    print(z,q,sum)\n",
    "    f_user_data_validation.close()\n",
    "    f_result.close()\n",
    "    print(\"precision: \", precision )\n",
    "    print(\"recall: \", recall)\n",
    "\n",
    "test('./data/tfidf_result.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on TfidfVectorizer in module sklearn.feature_extraction.text object:\n",
      "\n",
      "class TfidfVectorizer(CountVectorizer)\n",
      " |  Convert a collection of raw documents to a matrix of TF-IDF features.\n",
      " |  \n",
      " |  Equivalent to CountVectorizer followed by TfidfTransformer.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  input : string {'filename', 'file', 'content'}\n",
      " |      If 'filename', the sequence passed as an argument to fit is\n",
      " |      expected to be a list of filenames that need reading to fetch\n",
      " |      the raw content to analyze.\n",
      " |  \n",
      " |      If 'file', the sequence items must have a 'read' method (file-like\n",
      " |      object) that is called to fetch the bytes in memory.\n",
      " |  \n",
      " |      Otherwise the input is expected to be the sequence strings or\n",
      " |      bytes items are expected to be analyzed directly.\n",
      " |  \n",
      " |  encoding : string, 'utf-8' by default.\n",
      " |      If bytes or files are given to analyze, this encoding is used to\n",
      " |      decode.\n",
      " |  \n",
      " |  decode_error : {'strict', 'ignore', 'replace'}\n",
      " |      Instruction on what to do if a byte sequence is given to analyze that\n",
      " |      contains characters not of the given `encoding`. By default, it is\n",
      " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      " |      values are 'ignore' and 'replace'.\n",
      " |  \n",
      " |  strip_accents : {'ascii', 'unicode', None}\n",
      " |      Remove accents during the preprocessing step.\n",
      " |      'ascii' is a fast method that only works on characters that have\n",
      " |      an direct ASCII mapping.\n",
      " |      'unicode' is a slightly slower method that works on any characters.\n",
      " |      None (default) does nothing.\n",
      " |  \n",
      " |  analyzer : string, {'word', 'char'} or callable\n",
      " |      Whether the feature should be made of word or character n-grams.\n",
      " |  \n",
      " |      If a callable is passed it is used to extract the sequence of features\n",
      " |      out of the raw, unprocessed input.\n",
      " |  \n",
      " |  preprocessor : callable or None (default)\n",
      " |      Override the preprocessing (string transformation) stage while\n",
      " |      preserving the tokenizing and n-grams generation steps.\n",
      " |  \n",
      " |  tokenizer : callable or None (default)\n",
      " |      Override the string tokenization step while preserving the\n",
      " |      preprocessing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |  ngram_range : tuple (min_n, max_n)\n",
      " |      The lower and upper boundary of the range of n-values for different\n",
      " |      n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      " |      will be used.\n",
      " |  \n",
      " |  stop_words : string {'english'}, list, or None (default)\n",
      " |      If a string, it is passed to _check_stop_list and the appropriate stop\n",
      " |      list is returned. 'english' is currently the only supported string\n",
      " |      value.\n",
      " |  \n",
      " |      If a list, that list is assumed to contain stop words, all of which\n",
      " |      will be removed from the resulting tokens.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |      If None, no stop words will be used. max_df can be set to a value\n",
      " |      in the range [0.7, 1.0) to automatically detect and filter stop\n",
      " |      words based on intra corpus document frequency of terms.\n",
      " |  \n",
      " |  lowercase : boolean, default True\n",
      " |      Convert all characters to lowercase before tokenizing.\n",
      " |  \n",
      " |  token_pattern : string\n",
      " |      Regular expression denoting what constitutes a \"token\", only used\n",
      " |      if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
      " |      or more alphanumeric characters (punctuation is completely ignored\n",
      " |      and always treated as a token separator).\n",
      " |  \n",
      " |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly higher than the given threshold (corpus-specific\n",
      " |      stop words).\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  min_df : float in range [0.0, 1.0] or int, default=1\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly lower than the given threshold. This value is also\n",
      " |      called cut-off in the literature.\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  max_features : int or None, default=None\n",
      " |      If not None, build a vocabulary that only consider the top\n",
      " |      max_features ordered by term frequency across the corpus.\n",
      " |  \n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  vocabulary : Mapping or iterable, optional\n",
      " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      " |      indices in the feature matrix, or an iterable over terms. If not\n",
      " |      given, a vocabulary is determined from the input documents.\n",
      " |  \n",
      " |  binary : boolean, default=False\n",
      " |      If True, all non-zero term counts are set to 1. This does not mean\n",
      " |      outputs will have only 0/1 values, only that the tf term in tf-idf\n",
      " |      is binary. (Set idf and normalization to False to get 0/1 outputs.)\n",
      " |  \n",
      " |  dtype : type, optional\n",
      " |      Type of the matrix returned by fit_transform() or transform().\n",
      " |  \n",
      " |  norm : 'l1', 'l2' or None, optional\n",
      " |      Norm used to normalize term vectors. None for no normalization.\n",
      " |  \n",
      " |  use_idf : boolean, default=True\n",
      " |      Enable inverse-document-frequency reweighting.\n",
      " |  \n",
      " |  smooth_idf : boolean, default=True\n",
      " |      Smooth idf weights by adding one to document frequencies, as if an\n",
      " |      extra document was seen containing every term in the collection\n",
      " |      exactly once. Prevents zero divisions.\n",
      " |  \n",
      " |  sublinear_tf : boolean, default=False\n",
      " |      Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  vocabulary_ : dict\n",
      " |      A mapping of terms to feature indices.\n",
      " |  \n",
      " |  idf_ : array, shape = [n_features], or None\n",
      " |      The learned idf vector (global term weights)\n",
      " |      when ``use_idf`` is set to True, None otherwise.\n",
      " |  \n",
      " |  stop_words_ : set\n",
      " |      Terms that were ignored because they either:\n",
      " |  \n",
      " |        - occurred in too many documents (`max_df`)\n",
      " |        - occurred in too few documents (`min_df`)\n",
      " |        - were cut off by feature selection (`max_features`).\n",
      " |  \n",
      " |      This is only available if no vocabulary was given.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  CountVectorizer\n",
      " |      Tokenize the documents and count the occurrences of token and return\n",
      " |      them as a sparse matrix\n",
      " |  \n",
      " |  TfidfTransformer\n",
      " |      Apply Term Frequency Inverse Document Frequency normalization to a\n",
      " |      sparse matrix of occurrence counts.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The ``stop_words_`` attribute can get large and increase the model size\n",
      " |  when pickling. This attribute is provided only for introspection and can\n",
      " |  be safely removed using delattr or set to None before pickling.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TfidfVectorizer\n",
      " |      CountVectorizer\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      VectorizerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, raw_documents, y=None)\n",
      " |      Learn vocabulary and idf from training set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          an iterable which yields either str, unicode or file objects\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : TfidfVectorizer\n",
      " |  \n",
      " |  fit_transform(self, raw_documents, y=None)\n",
      " |      Learn vocabulary and idf, return term-document matrix.\n",
      " |      \n",
      " |      This is equivalent to fit followed by transform, but more efficiently\n",
      " |      implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          an iterable which yields either str, unicode or file objects\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix, [n_samples, n_features]\n",
      " |          Tf-idf-weighted document-term matrix.\n",
      " |  \n",
      " |  transform(self, raw_documents, copy=True)\n",
      " |      Transform documents to document-term matrix.\n",
      " |      \n",
      " |      Uses the vocabulary and document frequencies (df) learned by fit (or\n",
      " |      fit_transform).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          an iterable which yields either str, unicode or file objects\n",
      " |      \n",
      " |      copy : boolean, default True\n",
      " |          Whether to copy X and operate on the copy or perform in-place\n",
      " |          operations.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix, [n_samples, n_features]\n",
      " |          Tf-idf-weighted document-term matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  idf_\n",
      " |  \n",
      " |  norm\n",
      " |  \n",
      " |  smooth_idf\n",
      " |  \n",
      " |  sublinear_tf\n",
      " |  \n",
      " |  use_idf\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from CountVectorizer:\n",
      " |  \n",
      " |  get_feature_names(self)\n",
      " |      Array mapping from feature integer indices to feature name\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Return terms per document with nonzero entries in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array, sparse matrix}, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_inv : list of arrays, len = n_samples\n",
      " |          List of arrays of terms.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from VectorizerMixin:\n",
      " |  \n",
      " |  build_analyzer(self)\n",
      " |      Return a callable that handles preprocessing and tokenization\n",
      " |  \n",
      " |  build_preprocessor(self)\n",
      " |      Return a function to preprocess the text before tokenization\n",
      " |  \n",
      " |  build_tokenizer(self)\n",
      " |      Return a function that splits a string into a sequence of tokens\n",
      " |  \n",
      " |  decode(self, doc)\n",
      " |      Decode the input into a string of unicode symbols\n",
      " |      \n",
      " |      The decoding strategy depends on the vectorizer parameters.\n",
      " |  \n",
      " |  get_stop_words(self)\n",
      " |      Build or fetch the effective stop words list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "help(tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
