{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import json\n",
    "import codecs\n",
    "import jieba\n",
    "import numpy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_vector_dict(file_root, title_scale, doc_scale, min_df, max_df):\n",
    "    file = codecs.open(file_root, 'r', 'utf-8')\n",
    "    news_dict = json.load(file)\n",
    "    \n",
    "    # 停用词表\n",
    "    stop_file = codecs.open('./data/stop_words.txt', 'r', 'utf-8')\n",
    "    stop_list = stop_file.read().split('\\n')\n",
    "    stop_file.close()\n",
    "    \n",
    "    # 分词，在词之间加空格，重新组成文章\n",
    "    i = 0\n",
    "    title_array = []\n",
    "    doc_array = []\n",
    "    scale = max(int(title_scale / doc_scale), 1)\n",
    "    for news_id, news_info in news_dict.items():\n",
    "        title_text = news_info[0]\n",
    "        _title = jieba.lcut(title_text)\n",
    "        for w in _title[:]:\n",
    "            if w.split('.')[0].isdigit():\n",
    "                _title.remove(w)\n",
    "        title = ' '.join(_title)\n",
    "        title_array.append(title)\n",
    "        \n",
    "        doc_text = news_info[1]\n",
    "        _doc = jieba.lcut(doc_text)\n",
    "        for w in _doc[:]:\n",
    "            if w.split('.')[0].isdigit():\n",
    "                _doc.remove(w)\n",
    "        doc = ' '.join(_doc)\n",
    "        doc = title * scale + ' ' + doc\n",
    "        doc_array.append(doc)\n",
    "        \n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "    \n",
    "    \n",
    "    # tf-idf算法，文章转化为一个词向量\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df = min_df, max_df = max_df, stop_words = stop_list)\n",
    "    tfidf_vectorizer.fit(doc_array)\n",
    "    doc_matrix = tfidf_vectorizer.transform(doc_array)\n",
    "    news_matrix = doc_matrix.todense().tolist()\n",
    "    \n",
    "    # 构建news_key : vector字典\n",
    "    i = 0\n",
    "    _news_vector_dict = {}\n",
    "    for news_key, news_info in news_dict.items():\n",
    "        _news_vector_dict.setdefault(news_key, [numpy.asarray(news_matrix[i]), news_info[2]]) \n",
    "        i += 1\n",
    "    print(\"done.\")\n",
    "    return _news_vector_dict\n",
    "\n",
    "file_root = './data/_news_data_clean.json'\n",
    "# NOTE: scale = MAX(int(title_scale / doc_scale), 1)\n",
    "title_scale = 0.5\n",
    "doc_scale = 1.0 - title_scale\n",
    "min_df = 10\n",
    "max_df = 70\n",
    "_news_vector_dict = news_vector_dict(file_root, title_scale, doc_scale, min_df, max_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_trans(t):\n",
    "    day=int((t-1393603200)/86400)\n",
    "    return day\n",
    "\n",
    "def time_scale(day):\n",
    "    if day < 0:\n",
    "        day = 0\n",
    "    if day > 20:\n",
    "        day = 20\n",
    "#     return math.log(day+1)+1\n",
    "#     return math.log(math.log(day+1)+1)+1\n",
    "    return 1+0.05*day\n",
    "\n",
    "def user_vector_dict(_news_vector_dict):\n",
    "    file = codecs.open('./data/_user_data_training_clean.json', 'r', 'utf-8')\n",
    "    user_dict = json.load(file)\n",
    "    \n",
    "    j = 0\n",
    "    _user_vector_dict = {}\n",
    "    # 每一个用户\n",
    "    for user_key, user_info in user_dict.items():\n",
    "        # 该用户读过的所有新闻的向量和为用户向量\n",
    "        i = 0\n",
    "        vector_sum = numpy.matrix('0.0')\n",
    "        for user_news_key in user_info:\n",
    "            vector = numpy.matrix(_news_vector_dict[user_news_key][0])\n",
    "            vector_sum = vector + vector_sum\n",
    "            t_scale = time_scale(time_trans(_news_vector_dict[user_news_key][1]))\n",
    "            vector_sum = vector * t_scale + vector_sum\n",
    "            i += 1\n",
    "        if i != 0:\n",
    "            vector_sum /= i\n",
    "        #_user_vector_dict = {user_key: [user_vector, [all news_id read by this_user]]}\n",
    "        _user_vector_dict.setdefault(user_key, [numpy.asarray(list(vector_sum)[0]),list(user_info.keys())])\n",
    "    print(\"done\")\n",
    "    return _user_vector_dict\n",
    "\n",
    "\n",
    "_user_vector_dict = user_vector_dict(_news_vector_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_dist(_news_vector_dict, _user_vector_dict):\n",
    "    dist_result = {}\n",
    "    i=0\n",
    "    for user_id,user_v in _user_vector_dict.items():\n",
    "        i+=1\n",
    "        if i % 50 == 0:\n",
    "            print(i)\n",
    "        dist_list = []\n",
    "        read_news = user_v[1]\n",
    "        # calculate the Euclidean distance between each user and each news\n",
    "        for news_id,news_v in _news_vector_dict.items():\n",
    "            if news_id not in read_news:\n",
    "                tmp = user_v[0]-news_v[0]\n",
    "                dist= math.sqrt((tmp*tmp).sum())\n",
    "                dist_list.append([news_id, dist, time_trans(news_v[1])])\n",
    "                \n",
    "        dist_result.setdefault(user_id, dist_list)\n",
    "    print(\"done.\")\n",
    "    # dist_result = {user_id: for all news read by user->[news_id, distance, news_day]}\n",
    "    return dist_result\n",
    "\n",
    "\n",
    "dist_result = cal_dist(_news_vector_dict, _user_vector_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stress_date(day, dates):\n",
    "    # day: news_time\n",
    "    # dates: user click time\n",
    "    if day in dates:\n",
    "        return 8\n",
    "    if day+1 in dates:\n",
    "        return 5\n",
    "    if day+2 in dates:\n",
    "        return 3\n",
    "    return time_scale(day)\n",
    "    \n",
    "\n",
    "def get_dates():\n",
    "    f_user_data_validation = codecs.open('./data/_user_data_validation_clean.json', 'r', 'utf-8')\n",
    "    validation = json.load(f_user_data_validation)\n",
    "    user_dates = {}\n",
    "    for user, news in validation.items():\n",
    "        dates = []\n",
    "        for t in news.values():\n",
    "            day = time_trans(t)\n",
    "            if day not in dates:\n",
    "                dates.append(day)\n",
    "        user_dates.setdefault(user, dates)\n",
    "        \n",
    "    return user_dates\n",
    "\n",
    "def find_k_nbr_2(dist_result, dates, k):\n",
    "    # if user click news in day X, emphasis the weight of news release on X and X-1, X-2\n",
    "    result = {}\n",
    "    for user_id, dist in dist_result.items():\n",
    "        dist_list=[]\n",
    "        user_dates = dates[user_id]\n",
    "        num = len(user_dates)\n",
    "        for record in dist:\n",
    "            dist_list.append([record[0], record[1]/stress_date(record[2], user_dates)])\n",
    "        dist_list.sort(key=lambda x:x[1],reverse=False)\n",
    "        news = []\n",
    "        for j in range(k*num):\n",
    "            news.append(dist_list[j][0])\n",
    "        result.setdefault(user_id, news)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 15\n",
    "dates = get_dates()\n",
    "result_2 = find_k_nbr_2(dist_result, dates, k)\n",
    "file_output = codecs.open('./data/tfidf_result.json', 'w', 'utf-8')\n",
    "json.dump(result_2, file_output)\n",
    "file_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(result_root):\n",
    "    f_result = codecs.open(result_root, 'r', 'utf-8')\n",
    "    f_news_data = codecs.open('./data/_news_data.json', 'r', 'utf-8')\n",
    "    f_user_data_training = codecs.open('./data/_user_data_training_clean.json', 'r', 'utf-8')\n",
    "    f_user_data_validation = codecs.open('./data/_user_data_validation_clean.json', 'r', 'utf-8')\n",
    "    validation = json.load(f_user_data_validation)\n",
    "    training = json.load(f_user_data_training)\n",
    "    result = json.load(f_result)\n",
    "    news_data = json.load(f_news_data)\n",
    "    \n",
    "    z=0\n",
    "    q=0\n",
    "    user_num = 0\n",
    "    rec_num = 0\n",
    "    act_num = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    TP = 0\n",
    "    sum=0\n",
    "    news = []\n",
    "    for key, news_info in validation.items():\n",
    "        for news_id in news_info:\n",
    "            if news_id not in news:\n",
    "                news.append(news_id)\n",
    "    for key in validation:\n",
    "        user_num += 1\n",
    "        if key in result:\n",
    "            rec_num += len(result[key])\n",
    "        else:\n",
    "            continue\n",
    "        act_num += len(validation[key])\n",
    "#         TP = 0 \n",
    "        for news_id in result[key]:\n",
    "            q+=1\n",
    "    #         print(news_data[news_id][0])\n",
    "            if news_id in validation[key]:\n",
    "                TP+=1\n",
    "\n",
    "#         print(key)\n",
    "#         for a in validation[key]:\n",
    "#             print(news_data[a][0])\n",
    "#         print(\"\\n\")\n",
    "#         for a in training[key]:\n",
    "#             print(news_data[a][0])\n",
    "#         print(\"\\n\")\n",
    "#         sum+=TP\n",
    "        for a in validation[key]:\n",
    "            z+=1\n",
    "    precision += TP / rec_num\n",
    "    recall += TP / act_num\n",
    "#     precision = precision / user_num \n",
    "#     recall = recall / user_num\n",
    "    f1 += 2*(precision*recall)/(precision+recall)\n",
    "    print(\"number of records in validation set:\", z)\n",
    "    print(\"shot:                               \", q)\n",
    "    print(\"hits:                               \", TP)\n",
    "    f_user_data_validation.close()\n",
    "    f_result.close()\n",
    "    print(\"precision: \", precision )\n",
    "    print(\"recall:    \", recall)\n",
    "    print(\"f1:        \", f1)\n",
    "\n",
    "test('./data/tfidf_result.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
