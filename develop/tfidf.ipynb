{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import json\n",
    "import codecs\n",
    "import jieba\n",
    "import numpy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\54502\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.190 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "def news_vector_dict(file_root, title_scale, doc_scale, min_df, max_df):\n",
    "    file = codecs.open(file_root, 'r', 'utf-8')\n",
    "    news_dict = json.load(file)\n",
    "    \n",
    "    # 停用词表\n",
    "    stop_file = codecs.open('./data/stop_words.txt', 'r', 'utf-8')\n",
    "    stop_list = stop_file.read().split('\\n')\n",
    "    stop_file.close()\n",
    "    \n",
    "    # 分词，在词之间加空格，重新组成文章\n",
    "    i = 0\n",
    "    title_array = []\n",
    "    doc_array = []\n",
    "    scale = max(int(title_scale / doc_scale), 1)\n",
    "    for news_id, news_info in news_dict.items():\n",
    "        title_text = news_info[0]\n",
    "        _title = jieba.lcut(title_text)\n",
    "        for w in _title[:]:\n",
    "            if w.split('.')[0].isdigit():\n",
    "                _title.remove(w)\n",
    "        title = ' '.join(_title)\n",
    "        title_array.append(title)\n",
    "        \n",
    "        doc_text = news_info[1]\n",
    "        _doc = jieba.lcut(doc_text)\n",
    "        for w in _doc[:]:\n",
    "            if w.split('.')[0].isdigit():\n",
    "                _doc.remove(w)\n",
    "        doc = ' '.join(_doc)\n",
    "        doc = title * scale + ' ' + doc\n",
    "        doc_array.append(doc)\n",
    "        \n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "    \n",
    "    \n",
    "    # tf-idf算法，文章转化为一个词向量\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df = min_df, max_df = max_df, stop_words = stop_list)\n",
    "    tfidf_vectorizer.fit(doc_array)\n",
    "    doc_matrix = tfidf_vectorizer.transform(doc_array)\n",
    "    news_matrix = doc_matrix.todense().tolist()\n",
    "    \n",
    "    word_bag = {}\n",
    "    for key in tfidf_vectorizer.vocabulary_:\n",
    "        word_bag.setdefault(tfidf_vectorizer.vocabulary_[key], key)\n",
    "    \n",
    "    # 构建news_key : vector字典\n",
    "    i = 0\n",
    "    _news_vector_dict = {}\n",
    "    for news_key, news_info in news_dict.items():\n",
    "        _news_vector_dict.setdefault(news_key, [numpy.asarray(news_matrix[i]), news_info[2]])\n",
    "#         if i % 1000 == 0:\n",
    "#             print('i='+str(i))\n",
    "            \n",
    "#          #打印文章关键词和权重\n",
    "#         if i < 15:\n",
    "#             for j in range(len(news_matrix[i])):\n",
    "#                 if news_matrix[i][j] > 0:\n",
    "#                     print(word_bag[j] + \":\" + str(news_matrix[i][j]))\n",
    "#             print(doc_array[i])\n",
    "#             print('-------------------------')\n",
    "    \n",
    "        i += 1\n",
    "    print(\"done.\")\n",
    "    return _news_vector_dict\n",
    "\n",
    "file_root = './data/_news_data_clean.json'\n",
    "# NOTE: scale = MAX(int(title_scale / doc_scale), 1)\n",
    "title_scale = 0.5\n",
    "doc_scale = 1.0 - title_scale\n",
    "min_df = 5\n",
    "max_df = 30\n",
    "_news_vector_dict = news_vector_dict(file_root, title_scale, doc_scale, min_df, max_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def time_back_1(t):\n",
    "    a = int(t-1393603200)\n",
    "    return float(a / 86400 / 20)\n",
    "\n",
    "def user_vector_dict(_news_vector_dict):\n",
    "    file = codecs.open('./data/_user_data_training_clean.json', 'r', 'utf-8')\n",
    "    user_dict = json.load(file)\n",
    "    \n",
    "    j = 0\n",
    "    _user_vector_dict = {}\n",
    "    # 每一个用户\n",
    "    for user_key, user_info in user_dict.items():\n",
    "        # 该用户读过的所有新闻的向量和为用户向量\n",
    "        i = 0\n",
    "        vector_sum = numpy.matrix('0.0')\n",
    "        for user_news_key in user_info:\n",
    "            vector = numpy.matrix(_news_vector_dict[user_news_key][0])\n",
    "            vector_sum = vector + vector_sum\n",
    "#             time_scale = time_back(news_d[user_news_key][2])\n",
    "#             time_scale = 1\n",
    "#             vector_sum = vector * time_scale + vector_sum\n",
    "            i += 1\n",
    "        if i != 0:\n",
    "            vector_sum /= i\n",
    "        _user_vector_dict.setdefault(user_key, [numpy.asarray(list(vector_sum)[0]),list(user_info.keys())])\n",
    "#         j += 1\n",
    "#         if j % 1000 == 0:\n",
    "#             print('j='+str(j))\n",
    "#             print(vector_sum.tolist()[0][:10])\n",
    "    print(\"done\")\n",
    "    return _user_vector_dict\n",
    "\n",
    "\n",
    "_user_vector_dict = user_vector_dict(_news_vector_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "def time_trans(t):\n",
    "    day=int((t-1393603200)/86400)\n",
    "    return day\n",
    "\n",
    "def time_scale(day):\n",
    "    if day < 0:\n",
    "        day = 0\n",
    "    if day > 20:\n",
    "        day = 20\n",
    "#     return math.log(day+1)+1\n",
    "#     return math.log(math.log(day+1)+1)+1\n",
    "    return 1+0.05*day\n",
    "\n",
    "def cal_dist(_news_vector_dict, _user_vector_dict):\n",
    "    dist_result = {}\n",
    "    i=0\n",
    "    for user_id,user_v in _user_vector_dict.items():\n",
    "        i+=1\n",
    "        if i % 50 == 0:\n",
    "            print(i)\n",
    "        dist_list = []\n",
    "        read_news = user_v[1]\n",
    "        for news_id,news_v in _news_vector_dict.items():\n",
    "            if news_id not in read_news:\n",
    "                tmp = user_v[0]-news_v[0]\n",
    "                dist= math.sqrt((tmp*tmp).sum())\n",
    "                dist_list.append([news_id, dist, time_trans(news_v[1])])\n",
    "                \n",
    "        dist_result.setdefault(user_id, dist_list)\n",
    "#                 dist_list.append([news_id, dist/time_scale(news_v[1])])\n",
    "#         dist_list.sort(key=lambda x:x[1],reverse=False) \n",
    "#         news = []\n",
    "#         for j in range(k):\n",
    "#             news.append(dist_list[j][0])\n",
    "#         result.setdefault(user_id, news)\n",
    "    print(\"done.\")\n",
    "    return dist_result\n",
    "\n",
    "\n",
    "dist_result = cal_dist(_news_vector_dict, _user_vector_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_nbr_1(dist_result, k):\n",
    "    result = {}\n",
    "    for user_id, dist in dist_result.items():\n",
    "        dist_list=[]\n",
    "        for record in dist:\n",
    "            dist_list.append([record[0], record[1]/time_scale(record[2])])\n",
    "        dist_list.sort(key=lambda x:x[1],reverse=False)\n",
    "        news = []\n",
    "        for j in range(k):\n",
    "            news.append(dist_list[j][0])\n",
    "        result.setdefault(user_id, news)\n",
    "    return result\n",
    "        \n",
    "\n",
    "k = 50\n",
    "result_1 = find_k_nbr_1(dist_result, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_output = codecs.open('./data/tfidf_result.json', 'w', 'utf-8')\n",
    "json.dump(result_1, file_output)\n",
    "file_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stress_date(day, dates):\n",
    "    # day: news_time\n",
    "    # dates: user click time\n",
    "    if day in dates:\n",
    "        return 8\n",
    "    if day+1 in dates:\n",
    "        return 4.5\n",
    "    if day+2 in dates:\n",
    "        return 3\n",
    "    return time_scale(day)\n",
    "    \n",
    "\n",
    "def get_dates():\n",
    "    f_user_data_validation = codecs.open('./data/_user_data_validation_clean.json', 'r', 'utf-8')\n",
    "    validation = json.load(f_user_data_validation)\n",
    "    user_dates = {}\n",
    "    for user, news in validation.items():\n",
    "        dates = []\n",
    "        for t in news.values():\n",
    "            day = time_trans(t)\n",
    "            if day not in dates:\n",
    "                dates.append(day)\n",
    "        user_dates.setdefault(user, dates)\n",
    "        \n",
    "    return user_dates\n",
    "\n",
    "def find_k_nbr_2(dist_result, dates, k):\n",
    "    result = {}\n",
    "    for user_id, dist in dist_result.items():\n",
    "        dist_list=[]\n",
    "        user_dates = dates[user_id]\n",
    "        num = len(user_dates)\n",
    "        for record in dist:\n",
    "            dist_list.append([record[0], record[1]/stress_date(record[2], user_dates)])\n",
    "        dist_list.sort(key=lambda x:x[1],reverse=False)\n",
    "        news = []\n",
    "        for j in range(k*num):\n",
    "            news.append(dist_list[j][0])\n",
    "        result.setdefault(user_id, news)\n",
    "    return result\n",
    "    return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "dates = get_dates()\n",
    "result_2 = find_k_nbr_2(dist_result, dates, k)\n",
    "file_output = codecs.open('./data/tfidf_result.json', 'w', 'utf-8')\n",
    "json.dump(result_2, file_output)\n",
    "file_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1294\n",
      "5477 20610 1099\n",
      "precision:  0.05332362930616206\n",
      "recall:     0.20065729413912725\n",
      "f1:         0.08425652623912293\n"
     ]
    }
   ],
   "source": [
    "def test(result_root):\n",
    "    f_result = codecs.open(result_root, 'r', 'utf-8')\n",
    "    f_news_data = codecs.open('./data/_news_data.json', 'r', 'utf-8')\n",
    "    f_user_data_training = codecs.open('./data/_user_data_training_clean.json', 'r', 'utf-8')\n",
    "    f_user_data_validation = codecs.open('./data/_user_data_validation_clean.json', 'r', 'utf-8')\n",
    "    validation = json.load(f_user_data_validation)\n",
    "    training = json.load(f_user_data_training)\n",
    "    result = json.load(f_result)\n",
    "    news_data = json.load(f_news_data)\n",
    "    \n",
    "    z=0\n",
    "    q=0\n",
    "    user_num = 0\n",
    "    rec_num = 0\n",
    "    act_num = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    TP = 0\n",
    "    sum=0\n",
    "    news = []\n",
    "    for key, news_info in validation.items():\n",
    "        for news_id in news_info:\n",
    "            if news_id not in news:\n",
    "                news.append(news_id)\n",
    "    print(len(news))\n",
    "    for key in validation:\n",
    "        user_num += 1\n",
    "        if key in result:\n",
    "            rec_num += len(result[key])\n",
    "        else:\n",
    "            continue\n",
    "        act_num += len(validation[key])\n",
    "#         TP = 0 \n",
    "        for news_id in result[key]:\n",
    "            q+=1\n",
    "    #         print(news_data[news_id][0])\n",
    "            if news_id in validation[key]:\n",
    "                TP+=1\n",
    "\n",
    "#         print(key)\n",
    "#         for a in validation[key]:\n",
    "#             print(news_data[a][0])\n",
    "#         print(\"\\n\")\n",
    "#         for a in training[key]:\n",
    "#             print(news_data[a][0])\n",
    "#         print(\"\\n\")\n",
    "#         sum+=TP\n",
    "        for a in validation[key]:\n",
    "#             if time_trans(news_data[a][2]) < 10:\n",
    "            z+=1\n",
    "#         if TP == 0:\n",
    "#         print(result)\n",
    "        print(key)\n",
    "        for a in result[key]:\n",
    "            print(news_data[a][0], time_trans(news_data[a][2]))\n",
    "        print(\"\\n\")\n",
    "        for a in validation[key]:\n",
    "            print(news_data[a][0], time_trans(news_data[a][2]),time_trans(validation[key][a]))\n",
    "        print(\"\\n\")\n",
    "\n",
    "#             for a in training[key]:\n",
    "#                 print(news_data[a][0], time_trans(news_data[a][2]),time_trans(training[key][a]))\n",
    "#             print(\"\\n\")\n",
    "#             break\n",
    "    \n",
    "#         print(precision, recall)\n",
    "    precision += TP / rec_num\n",
    "    recall += TP / act_num\n",
    "#     precision = precision / user_num \n",
    "#     recall = recall / user_num\n",
    "    f1 += 2*(precision*recall)/(precision+recall)\n",
    "    print(z,q,TP)\n",
    "    f_user_data_validation.close()\n",
    "    f_result.close()\n",
    "    print(\"precision: \", precision )\n",
    "    print(\"recall:    \", recall)\n",
    "    print(\"f1:        \", f1)\n",
    "\n",
    "test('./data/tfidf_result.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "news_keys = []\n",
    "i = 0\n",
    "for news_key in _news_vector_dict:\n",
    "    news_keys.append(news_key)\n",
    "\n",
    "    \n",
    "file = codecs.open('./data/_user_data_training_clean.json', 'r', 'utf-8')\n",
    "f_news_data = codecs.open('./data/_news_data_clean.json', 'r', 'utf-8')\n",
    "user_news_dict = json.load(file)\n",
    "news_data = json.load(f_news_data)\n",
    "\n",
    "result = {}\n",
    "i = 0\n",
    "lens = []\n",
    "pair = []\n",
    "for user_key in user_news_dict:\n",
    "    dist = n[2*i][0].tolist()\n",
    "    indices = n[2*i+1][0].tolist()\n",
    "    pair = []\n",
    "    for m in range(len(indices)):\n",
    "        mth = indices[m]\n",
    "        news_id = news_keys[mth]\n",
    "        if news_id in user_news_dict[user_key] or news_data[news_id][2] < 1393603200:\n",
    "            continue\n",
    "    \n",
    "        time_ratio = time_scale(news_data[news_id][2])\n",
    "        #print(time_ratio, dist[m], dist[m]*time_ratio)\n",
    "        pair.append([dist[m] * time_ratio, news_id])\n",
    "    \n",
    "    pair.sort(key=lambda x:x[0],reverse=True)\n",
    "    user_news_keys = []\n",
    "    for k in range(20):\n",
    "        user_news_keys.append(pair[k][1])\n",
    "    result.setdefault(user_key, user_news_keys)\n",
    "    if i < 10:\n",
    "        for p in pair:\n",
    "            print(p[0], time_scale(news_data[p[1]][2]))\n",
    "        print(\"\\n\")\n",
    "    \n",
    "#         print(len(result[user_key]))\n",
    "#         print(result[user_key])\n",
    "#         print(user_news_dict[user_key])\n",
    "    i += 1\n",
    "    lens.append(len(result[user_key]))\n",
    "lens.sort()\n",
    "print(lens[:10])\n",
    "\n",
    "\n",
    "\n",
    "# news_keys = []\n",
    "# i = 0\n",
    "# for news_key in _news_vector_dict:\n",
    "#     news_keys.append(news_key)\n",
    "\n",
    "    \n",
    "# file = codecs.open('./data/_user_data_training_clean.json', 'r', 'utf-8')\n",
    "# user_news_dict = json.load(file)\n",
    "\n",
    "# result = {}\n",
    "# i = 0\n",
    "# lens = []\n",
    "# for user_key in user_news_dict:\n",
    "#     indices = n[2*i+1][0].tolist()\n",
    "#     user_news_keys = []\n",
    "#     for index in indices:\n",
    "#         user_news_key = news_keys[index]\n",
    "#         if user_news_key not in user_news_dict[user_key]:\n",
    "#             user_news_keys.append(user_news_key)\n",
    "#     result.setdefault(user_key, user_news_keys)\n",
    "# #     if i < 100:\n",
    "# #         print(len(result[user_key]))\n",
    "# #         print(result[user_key])\n",
    "# #         print(user_news_dict[user_key])\n",
    "#     i += 1\n",
    "#     lens.append(len(result[user_key]))\n",
    "# print(lens[:10])\n",
    "# lens.sort()\n",
    "# print(lens[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_output = codecs.open('./data/tfidf_result.json', 'w', 'utf-8')\n",
    "json.dump(result, file_output)\n",
    "file_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "846 10230 357\n",
      "precision:  0.034897360703812254\n",
      "recall:  0.06877895738997042\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=numpy.asarray([1,2,3])\n",
    "b=numpy.asarray([2,3,4])\n",
    "print(((b-a)*(b-a)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([2])\n"
     ]
    }
   ],
   "source": [
    "a={\"1\":2}\n",
    "print(a.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
